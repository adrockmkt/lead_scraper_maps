# Lead Scraper Maps â€“ Grupo 3 (Alto Ticket)

Este projeto Ã© um **scraper tÃ©cnico e educativo** para coleta estruturada de dados **publicamente disponÃ­veis** no Google Maps (via Google Places API), com foco em **anÃ¡lise de mercado local** e **estudos de concorrÃªncia**.

Ele foi projetado para execuÃ§Ã£o **local**, com controle de custo, cache em SQLite e geraÃ§Ã£o de dados estruturados para **anÃ¡lise estratÃ©gica**, nÃ£o para envio automÃ¡tico de comunicaÃ§Ãµes.

---

## ğŸ¯ Objetivo do Projeto

O objetivo deste projeto Ã© demonstrar uma arquitetura prÃ¡tica para coleta, enriquecimento e classificaÃ§Ã£o de dados pÃºblicos de negÃ³cios locais, atendendo aos seguintes critÃ©rios tÃ©cnicos:

- Empresa com **site prÃ³prio**
- PresenÃ§a ativa no **Google Maps**
- AtuaÃ§Ã£o em **nichos de alto ticket** (problema â†’ soluÃ§Ã£o)
- LocalizaÃ§Ã£o em regiÃµes com **concorrÃªncia relevante** (proxy de Ads caros)
- PreferÃªncia por **email corporativo** (separando casos sem email)

---

## ğŸ§± Arquitetura Geral

Pipeline resumido:

1. **Google Maps (Places API â€“ Text Search)**
   - Busca por *nicho + bairro + cidade*
   - Curitiba/PR + RegiÃ£o Metropolitana

2. **Enriquecimento (Place Details)**
   - Site
   - Telefone
   - EndereÃ§o
   - Componentes de endereÃ§o (bairro)

3. **Crawling leve do site**
   - Home + pÃ¡ginas de contato
   - ExtraÃ§Ã£o de emails
   - ClassificaÃ§Ã£o: corporativo vs genÃ©rico

4. **Scoring automÃ¡tico (0â€“100)**
   - Nicho
   - ConcorrÃªncia
   - RegiÃ£o
   - PresenÃ§a de email corporativo

5. **PersistÃªncia e saÃ­da**
   - Cache SQLite
   - CSVs incrementais por status

---

## ğŸ“ Estrutura do Projeto

```
lead_scraper_maps/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ places_client.py
â”‚   â”œâ”€â”€ site_crawler.py
â”‚   â”œâ”€â”€ scoring.py
â”‚   â””â”€â”€ storage.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sqlite_cache.db
â””â”€â”€ outputs/
    â”œâ”€â”€ leads_qualificados.csv
    â”œâ”€â”€ leads_sem_email.csv
    â””â”€â”€ leads_descartados.csv
```

---

## âš™ï¸ Requisitos

- Python **3.9+**
- Conta Google Cloud com **Places API habilitada**
- Ambiente virtual (`venv`)

---

## ğŸ” ConfiguraÃ§Ã£o

### 1. Criar o ambiente virtual

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 3. Configurar `.env`

```env
GOOGLE_MAPS_API_KEY=COLE_SUA_API_KEY
REQUEST_DELAY=1.2
USER_AGENT=Mozilla/5.0
SQLITE_DB_PATH=data/sqlite_cache.db
```

âš ï¸ **Nunca versionar o `.env`**.

---

## â–¶ï¸ ExecuÃ§Ã£o

Rodar o pipeline completo:

```bash
python main.py
```

Na primeira execuÃ§Ã£o:
- Mais lenta (cache vazio)
- Maior volume de chamadas Ã  API

ExecuÃ§Ãµes seguintes:
- Muito mais rÃ¡pidas
- Uso intensivo de cache

---

## ğŸ“¤ Outputs Gerados

Todos os arquivos sÃ£o gerados incrementalmente em `outputs/`:

- **leads_qualificados.csv**
  - Score â‰¥ 70
  - Prontos para CRM / Kit / abordagem direta

- **leads_sem_email.csv**
  - Empresas com site, mas sem email corporativo
  - Ideais para contato via telefone ou WhatsApp

- **leads_descartados.csv**
  - Fora do perfil estratÃ©gico

---

## ğŸ§  EstratÃ©gia de Uso

Este projeto tem carÃ¡ter **tÃ©cnico e demonstrativo**. Ele pode ser utilizado para:

- Estudos de mercado local
- AnÃ¡lise de concorrÃªncia por regiÃ£o e nicho
- AvaliaÃ§Ã£o de maturidade digital (site, presenÃ§a local, canais)
- Prototipagem de pipelines de dados

Qualquer uso para comunicaÃ§Ã£o, prospecÃ§Ã£o ou marketing deve respeitar integralmente:
- LGPD
- Termos de Uso do Google
- PolÃ­ticas de consentimento das plataformas de email/CRM

---

## ğŸ”’ Compliance e Boas PrÃ¡ticas

- Utiliza exclusivamente **dados pÃºblicos**
- Consome APIs oficiais (Google Places API)
- Crawling limitado e nÃ£o intrusivo
- Rate limit e cache aplicados
- NÃ£o realiza envio de emails, mensagens ou automaÃ§Ãµes de contato

---

## ğŸ’° Limites de Custo e Boas PrÃ¡ticas de API

Este projeto utiliza a **Google Places API**, que possui modelo de cobranÃ§a por volume de requisiÃ§Ãµes. Para evitar custos inesperados, siga rigorosamente as boas prÃ¡ticas abaixo.

### ğŸ“Œ RecomendaÃ§Ãµes de Controle de Custo

- **Utilize sempre cache local (SQLite)**
  - O projeto jÃ¡ evita chamadas duplicadas para o mesmo `place_id` e domÃ­nio
  - NÃ£o apague o arquivo `data/sqlite_cache.db` entre execuÃ§Ãµes

- **Execute por nichos especÃ­ficos**
  - Evite rodar todos os nichos simultaneamente
  - Valide os resultados de um nicho antes de avanÃ§ar para o prÃ³ximo

- **Monitore o consumo no Google Cloud Console**
  - Acesse: *Billing â†’ Reports*
  - Filtre por *Places API*

- **Aplique limites de execuÃ§Ã£o**
  - Reduza temporariamente a lista de bairros em `config.py` para testes
  - Ajuste `MAX_PAGES_PER_QUERY` no `.env` durante validaÃ§Ã£o inicial

### ğŸ” Boas PrÃ¡ticas de SeguranÃ§a da API Key

- Restrinja a API Key para:
  - **Places API apenas**
  - (Opcional) IP local durante desenvolvimento

- Nunca versionar ou expor a chave em repositÃ³rios pÃºblicos

### ğŸ“Š Cotas gratuitas e modelo de cobranÃ§a da Google Maps API

Este projeto utiliza exclusivamente endpoints da **Google Places API** (Text Search e Place Details), que possuem **cota gratuita mensal** oferecida pela Google Maps Platform.

Atualmente, a Google disponibiliza:

- **Places API â€“ Text Search**: atÃ© **5.000 requisiÃ§Ãµes/mÃªs sem custo**
- **Places Details (campos bÃ¡sicos)**: atÃ© **5.000 requisiÃ§Ãµes/mÃªs sem custo**

As cotas sÃ£o **renovadas mensalmente** e sÃ£o **independentes por tipo de requisiÃ§Ã£o (SKU)**.

No cenÃ¡rio de uso deste projeto â€” execuÃ§Ã£o manual, escopo regional (Curitiba e RegiÃ£o Metropolitana), cache persistente em SQLite e execuÃ§Ã£o por nicho â€” Ã© possÃ­vel operar **integralmente dentro do free tier**, sem geraÃ§Ã£o de cobranÃ§a.

Mesmo assim, recomenda-se fortemente a criaÃ§Ã£o de um **budget mensal** no Google Cloud Console para monitoramento e alertas preventivos.

### âš ï¸ ObservaÃ§Ã£o Importante

O projeto foi desenhado para **execuÃ§Ã£o consciente e incremental**. Ele nÃ£o deve ser utilizado como crawler massivo ou contÃ­nuo.

A responsabilidade pelo uso da API, custos gerados e conformidade com os termos do Google Ã© sempre do operador do script.

---

## ğŸš€ EvoluÃ§Ãµes Planejadas

- IntegraÃ§Ã£o com Google Ads Keyword Planner (CPC real)
- DetecÃ§Ã£o de anÃºncios ativos no SERP
- ClusterizaÃ§Ã£o avanÃ§ada por renda
- ExportaÃ§Ã£o direta para CRMs
- ExecuÃ§Ã£o automatizada (cron / servidor)

---

## ğŸ‘¤ Autor

Projeto open-source desenvolvido pela **Ad Rock Digital Mkt** como referÃªncia tÃ©cnica para arquiteturas de coleta e anÃ¡lise de dados locais.

Este repositÃ³rio Ã© disponibilizado para fins **educacionais e tÃ©cnicos**.